\documentclass[Main]{subfiles}

\begin{document}
% Methods



The sentiment analysis uses the Python modules nltk, 

3rd party libraries ``application\_only\_auth'' and ``twokenize''

Important development tools used includes \textit{Git} for version control, \textit{Sublime Text} and \textit{Emacs} for editing source code, \textit{flake8} for checking Python code, and \textit{Slack} for collaboration.



\textbf{Tokenizing the tweets}
In stead of using a simple `split' function on the tweets, a tokenizer designed for tweets is used. Two different tokenizers have been considered; ``HappyFunTokenizing'' from [ref: ] \href{http://sentiment.christopherpotts.net/tokenizing.html}{\tt C. Potts, Stanford Linguistics} and ``ark-twokenize'' from a Carnegie Mellon University research group \href{http://www.ark.cs.cmu.edu/TweetNLP/}{\tt Tweet NLP}

The chosen tokenizer, ``ark-twokenize'', performs visibly better on links, which is an important feature. E.g. [HappyFun] will return the following 
\[u'http', u':/', u'/', u't', u'.', u'co', u'/', u'reupcsoekj'\], whereas [ARK] will return \[u'http://t.co/rEuPcSOekJ'\].

Tokenizing is important, since the classification features will be based on the tokens. 





\subsection{Sentiment analysis}


With inspiration from [ref...........], classification has been made with three classes; ``positive'', ``negative'' and ``objective''.

Collection of training material has been done with the Twitter API, where 3 kind of queries were made. Two of the queries were made with the emoticons ``:)'', and ``:('' for querying positive and negative tweets respectively. For each emoticon 10.000 tweets were collected. For obtaining objective tweets, tweets from three major news companies (NYTimes, BBCWorld and Reuters) were queried. In total just shy of 10.000 news tweets were collected. 

\textbf{Feature extraction}

Before using the tweets two things are done; tokenization and filtering.



\textbf{Classification}
 




\end{document}
