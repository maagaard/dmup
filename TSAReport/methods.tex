\documentclass[Main]{subfiles}

\begin{document}
% Methods



The sentiment analysis uses the Python modules nltk, 

3rd party libraries ``application\_only\_auth'' and ``twokenize''

Important development tools used includes \textit{Git} for version control, \textit{Sublime Text} and \textit{Emacs} for editing source code, \textit{flake8} for checking Python code, and \textit{Slack} for collaboration.




\subsection{Sentiment analysis}


With inspiration from a paper on Twitter as corpus data for sentiment analysis \cite{pak2010twitter}, classification has been made with three classes; ``positive'', ``negative'' and ``objective''.

Collection of training material has been done with the Twitter API, where 3 kind of queries were made. Two of the queries were made with the emoticons ``:)'', and ``:('' for querying positive and negative tweets respectively. For each emoticon 10.000 tweets were collected. For obtaining objective tweets, tweets from three major news companies (NYTimes, BBCWorld and Reuters) were queried. In total just shy of 10.000 news tweets were collected. 

\textbf{Feature extraction}

Before using the tweets two things are done; tokenization and filtering.

\textit{Tokenization and filtering}
In stead of using a simple `split' function on the tweets, a tokenizer designed for tweets is used. Two different tokenizers have been considered; ``HappyFunTokenizing'' from \href{http://sentiment.christopherpotts.net/tokenizing.html}{\tt C. Potts, Stanford Linguistics} and ``ark-twokenize'' from a Carnegie Mellon University research group \href{http://www.ark.cs.cmu.edu/TweetNLP/}{\tt Tweet NLP}

The chosen tokenizer, ``ark-twokenize'', performs visibly better on links, which is an important feature. E.g. [HappyFun] will return the following 
\[u'http', u':/', u'/', u't', u'.', u'co', u'/', u'reupcsoekj'\], whereas [ARK] will return \[u'http://t.co/rEuPcSOekJ'\].

The way the tokenizer tokenizes links are important, since the classification features will be based on the tokens. Before turning tokens in to features, the ``non-text-tokens'' are filtered. The filtering removes all non-text such as links, hashtags, retweet-tags and usernames, emoticons are kept. 


\textbf{Classification}

The sentiment classifier used for the classification is a Na√Øve Bayes Classifier from the nltk module. The classifier is trained with the collected data, although not all the data is used, since the training, and later classification, will take far too long for this application to function. The order of the data is shuffled before used for training. 



\subsection{Web Front-end}
When a sentiment analysis has been run, the result needs to be presented to the user. 
This is done on the web front-end, written using the ``Flask'' microframework. 
The framework fits the application nicely, as it does only the bare minimum of what is needed to run our application on a web server.
The front-end can be seen as a sort of ``main''-function, that ties everything together into an easy to use package.
A screenshot of the website can be seen on \autoref{fig:website}.


\end{document}
